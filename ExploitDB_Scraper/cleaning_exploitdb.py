import pandas as pd
import re
import emoji
import html
# from deep_translator import GoogleTranslator

# Load DataFrame
DF_PATH = 'exploitdb_cve.csv'
# DF_PATH = 'exploitdb_03_05_cleaned_nolinks.csv'
OUTPUT_PATH = 'exploitdb_03_05_cleaned_nolinks.csv'

df = pd.read_csv(DF_PATH)

# =================================
md_link_ref = re.compile(r"\[([^\]]+?)\]\(((?:https?://|mailto:)[^\s)]+)\)")
url_ref         = re.compile(r"(?:https?://|http://|www\.)\S+")
zero_width_re   = re.compile(r"[\u200b-\u200d\u2060]")
html_breaks_re  = re.compile(r"(<br\s*/?>|&nbsp;)", re.IGNORECASE)
cve_re          = re.compile(r"\b(cve-\d{4}-\d{4,})\b", re.IGNORECASE)
code_block_re   = re.compile(r"```[\s\S]*?```")
md_punctuation  = re.compile(r"(\*\*|\*|__|_|~~|`)(.*?)\1")
heading_hash_re = re.compile(r"^#{1,6}\s+", re.MULTILINE)
excess_punct_re = re.compile(r"([^\w\s])\1{2,}")
hashtag_re      = re.compile(r"#\s*(?=\w)")
# quote_re       = re.compile(r'""[^"]*""')

# =================================
def remove_emoji(text: str) -> str:
    return emoji.replace_emoji(text, replace='')

def normalize_extra_tokens(text: str) -> str:
    text = cve_re.sub(lambda m: m.group(1).upper(), text)
    text = html.unescape(text)
    text = html_breaks_re.sub(' ', text)
    text = zero_width_re.sub('', text)
    text = code_block_re.sub(' <CODE> ', text)
    text = md_punctuation.sub(r"\2", text)
    text = heading_hash_re.sub('', text)
    text = excess_punct_re.sub(r"\1", text)
    text = re.sub(r'("{2,})([^"]+?)\1', r'\2', text)
    text = remove_emoji(text)
    return text.strip()

spaced_url_re = re.compile(r"(?:https?://|http://|www\.)\s*\S*")

def remove_links(text: str) -> str:
    text = spaced_url_re.sub(' <URL> ', text)
    return text

def remove_hashtags(text: str) -> str:
    return hashtag_re.sub('', text)

def remove_usernames_and_metadata(text: str) -> str:
    text = re.sub(r'@\w+', '', text)
    text = md_link_ref.sub('', text)
    return text

def clean_text(text: str) -> str:
    if pd.isna(text):
        return ''
    text = str(text)
    text = normalize_extra_tokens(text)
    text = remove_links(text)
    text = remove_hashtags(text)
    text = remove_usernames_and_metadata(text)
    return re.sub(r'\s+', ' ', text).strip()

print('Cleaning exploitdb text...')
df['text'] = df['text'].apply(clean_text)

# df = df[df['text'].astype(bool)]

print(f"Saving cleaned data to {OUTPUT_PATH} (shape: {df.shape})")
df.to_csv(OUTPUT_PATH, index=False)
